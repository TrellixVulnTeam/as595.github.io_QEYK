{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================================================\n",
    "\n",
    "&lsaquo; GPMIntro.ipynb  &rsaquo;\n",
    "Copyright (C) &lsaquo; 2017 &rsaquo;  &lsaquo; Anna Scaife - anna.scaife@manchester.ac.uk &rsaquo;\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\n",
    "==============================================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**[AMS - 170402]**  Notebook created for **SKA-SA Newton Big Data Summer School, Cape Town, April 2017**<br>\n",
    "**[AMS - 170823]**  Notebook updated for **TIARA Astrostatistics Summer School, Taipei, August 2017**<br>\n",
    "**[AMS - 180528]**  Notebook updated for **JEDI Madagascar, Nosy Be, May 2018**\n",
    "\n",
    "This notebook uses GPM to predict a signal.  It recreates some of the plots from Roberts et al. 2013 (http://www.robots.ox.ac.uk/~sjrob/Pubs/Phil.%20Trans.%20R.%20Soc.%20A-2013-Roberts-.pdf). It is a teaching resource and is accompanied by  the lecture \"Can you Predict the Future..?\".\n",
    "\n",
    "All Python libraries used in this example can be installed using **pip**.\n",
    "\n",
    "---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start let's specify that we want our figures to appear embedded in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's import all the libraries we need..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the covariance kernel a squared-exponential,\n",
    "\n",
    "$k(x_1,x_2) = h^2 \\exp{ \\left( \\frac{-(x_1 - x_2)^2}{\\lambda^2} \\right)}$,\n",
    "\n",
    "just like Eq. 3.11 in Roberts et al. (2012).\n",
    "\n",
    "http://www.robots.ox.ac.uk/~sjrob/Pubs/Phil.%20Trans.%20R.%20Soc.%20A-2013-Roberts-.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_kernel(x1,x2,h,lam):\n",
    "    \"\"\"\n",
    "    Squared-Exponential covariance kernel\n",
    "    \"\"\"\n",
    "    k12 = h**2*np.exp(-1.*(x1 - x2)**2/lam**2)\n",
    "    \n",
    "    return k12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this kernel to calculate  the value of each element in our covariance matrix:\n",
    "\n",
    "$\\mathbf{K(x,x)} = \\left(\n",
    "\\begin{array}{cccc}\n",
    "k(x_1,x_1) & k(x_1,x_2) & ... & k(x_1,x_n) \\\\\n",
    "k(x_2,x_1) & k(x_2,x_2) & ... & k(x_2,x_n) \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "k(x_n,x_1) & k(x_n,x_2) & ... & k(x_n,x_n) \n",
    "\\end{array}\n",
    "\\right).$\n",
    "\n",
    "We can then populate a covariance matrix, $K(\\mathbf{x},\\mathbf{x})$, for our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_K(x, h, lam):\n",
    "    \n",
    "    \"\"\"\n",
    "    Make covariance matrix from covariance kernel\n",
    "    \"\"\"\n",
    "    \n",
    "    # for a data array of length x, make a covariance matrix x*x:\n",
    "    K = np.zeros((len(x),len(x)))\n",
    "    \n",
    "    for i in range(0,len(x)):\n",
    "        for j in range(0,len(x)):\n",
    "            \n",
    "            # calculate value of K for each separation:\n",
    "            K[i,j] = cov_kernel(x[i],x[j],h,lam)\n",
    "            \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this kernel we can then recreate Fig. 5 from Roberts et al. (2012)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annascaife/SRC/GITHUB/p3env/lib/python3.6/site-packages/matplotlib/figure.py:98: MatplotlibDeprecationWarning: \n",
      "Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  \"Adding an axes using the same arguments as a previous axes \"\n"
     ]
    }
   ],
   "source": [
    "# make an array of 200 evenly spaced positions between 0 and 20:\n",
    "x1 = np.arange(0, 20.,0.01)\n",
    "    \n",
    "for i in range(0,3):\n",
    "    \n",
    "    h = 1.0\n",
    "    \n",
    "    if (i==0): lam = 0.1\n",
    "    if (i==1): lam = 1.0\n",
    "    if (i==2): lam = 5.0\n",
    "        \n",
    "    # make a covariance matrix:\n",
    "    K = make_K(x1,h,lam)\n",
    "    \n",
    "    # five realisations:\n",
    "    for j in range(0,5):\n",
    "        \n",
    "        # draw samples from a co-variate Gaussian distribution, N(0,K):\n",
    "        y1 = np.random.multivariate_normal(np.zeros(len(x1)),K)\n",
    "    \n",
    "        tmp2 = '23'+str(i+3+1)\n",
    "        pl.subplot(int(tmp2))\n",
    "        pl.plot(x1,y1)\n",
    "        \n",
    "        \n",
    "    tmp1 = '23'+str(i+1)\n",
    "    pl.subplot(int(tmp1))\n",
    "    pl.imshow(K)\n",
    "    pl.title(r\"$\\lambda = $\"+str(lam))\n",
    "    \n",
    "    \n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then take the final realization, which has $\\lambda = 5$, and select 5 points from it randomly we can calculate the posterior mean and variance at every point based on those five input data. \n",
    "\n",
    "The mean and variance are given by Eq. 3.8 & 3.9 in Roberts et al. (2012) or Eq. 2.25 & 2.26 in Rasmussen & Williams.\n",
    "\n",
    "First let's select our  **training data points** and our **test data points**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of training points\n",
    "nx_training = 5\n",
    "\n",
    "# randomly select the training points:\n",
    "tmp = np.random.uniform(low=0.0, high=2000.0, size=nx_training)\n",
    "tmp = tmp.astype(int)\n",
    "\n",
    "condition = np.zeros_like(x1)\n",
    "for i in tmp: condition[i] = 1.0\n",
    "    \n",
    "y_train = y1[np.where(condition==1.0)]\n",
    "x_train = x1[np.where(condition==1.0)]\n",
    "y_test = y1[np.where(condition==0.0)]\n",
    "x_test = x1[np.where(condition==0.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use our **training data points** to define a covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the covariance matrix:\n",
    "K = make_K(x_train,h,lam)\n",
    "\n",
    "# take the inverse:\n",
    "iK = np.linalg.inv(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of our **test data points** we can then make a prediction of the value at $x_{\\ast}$ and the uncertainly (standard deviation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu=[];sig=[]\n",
    "for xx in x_test:\n",
    "\n",
    "    # find the 1d covariance matrix:\n",
    "    K_x = cov_kernel(xx, x_train, h, lam)\n",
    "    \n",
    "    # find the kernel for (x,x):\n",
    "    k_xx = cov_kernel(xx, xx, h, lam)\n",
    "    \n",
    "    # calculate the posterior mean and variance:\n",
    "    mu_xx = np.dot(K_x.T,np.dot(iK,y_train))\n",
    "    sig_xx = k_xx - np.dot(K_x.T,np.dot(iK,K_x))\n",
    "    \n",
    "    mu.append(mu_xx)\n",
    "    sig.append(np.sqrt(np.abs(sig_xx))) # note sqrt to get stdev from variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu and sig are currently lists - turn them into numpy arrays:\n",
    "mu=np.array(mu);sig=np.array(sig)\n",
    "\n",
    "# make some plots:\n",
    "pl.xkcd():\n",
    "# left-hand plot\n",
    "ax = pl.subplot(121)\n",
    "pl.scatter(x_train,y_train)  # plot the training points\n",
    "pl.plot(x1,y1,ls=':')        # plot the original data they were drawn from\n",
    "pl.title(\"Input\")\n",
    "\n",
    "# right-hand plot\n",
    "ax = pl.subplot(122)\n",
    "pl.plot(x_test,mu,ls='-')     # plot the predicted values\n",
    "pl.plot(x_test,y_test,ls=':') # plot the original values\n",
    "\n",
    "\n",
    "# shade in the area inside a one standard deviation bound:\n",
    "ax.fill_between(x_test,mu-sig,mu+sig,facecolor='lightgrey', lw=0, interpolate=True)\n",
    "pl.title(\"Predicted\")\n",
    "\n",
    "pl.scatter(x_train,y_train)  # plot the training points\n",
    "\n",
    "# display the plot:\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Note: Depending on  the selection of training points you might want to specify some axis ranges for these plots]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--END--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
