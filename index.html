---
layout: default
---

<div>
  The notes are for the lectures and tutorials on an Introduction to Machine Learning at the  <a href="https://indico.cern.ch/event/769356/page/15188-lecturers">CERN School of Computing 2019</a>.
  <br>
  Questions and comments on these notes can be sent to <a href="http://cs.stanford.edu/people/karpathy/">Anna Scaife</a>, or entered as an issue on the <a href="https://github.com/as595/as595.github.io">github repo</a>.
  <br>
</div>

<div class="home">
  <div class="materials-wrap">

	 <!-- hardcoding items here to force a specific order -->
    <div class="module-header">Module 1: Neural Networks</div>

    <div class="materials-item">
      <a href="classification/">
        Image Classification: Data-driven Approach, k-Nearest Neighbor, train/val/test splits
      </a>
      <div class="kw">
        L1/L2 distances, hyperparameter search, cross-validation
      </div>
    </div>

    <div class="materials-item">
      <a href="linear-classify/">
         Linear classification: Support Vector Machine, Softmax
      </a>
      <div class="kw">
        parameteric approach, bias trick, hinge loss, cross-entropy loss, L2 regularization, web demo
      </div>
    </div>

    <div class="materials-item">
      <a href="optimization-1/">
        Optimization: Stochastic Gradient Descent
      </a>
      <div class="kw">
        optimization landscapes, local search, learning rate, analytic/numerical gradient
      </div>
    </div>

    <div class="materials-item">
      <a href="optimization-2/">
        Backpropagation, Intuitions
      </a>
      <div class="kw">
        chain rule interpretation, real-valued circuits, patterns in gradient flow
      </div>
    </div>

    <div class="materials-item">
      <a href="neural-networks-1/">
          Neural Networks Part 1: Setting up the Architecture
      </a>
      <div class="kw">
        model of a biological neuron, activation functions, neural net architecture, representational power
      </div>
    </div>

    <div class="materials-item">
      <a href="neural-networks-2/">
          Neural Networks Part 2: Setting up the Data and the Loss
      </a>
      <div class="kw">
          preprocessing, weight initialization, batch normalization, regularization (L2/dropout), loss functions
      </div>
    </div>

    <div class="materials-item">
      <a href="neural-networks-3/">
        Neural Networks Part 3: Learning and Evaluation
      </a>
      <div class="kw">
        gradient checks, sanity checks, babysitting the learning process, momentum (+nesterov), second-order methods, Adagrad/RMSprop, hyperparameter optimization, model ensembles
      </div>
    </div>

    <div class="materials-item">
      <a href="neural-networks-case-study/">
          Putting it together: Minimal Neural Network Case Study
      </a>
      <div class="kw">
        minimal 2D toy data example
      </div>
    </div>

    <div class="module-header">Module 2: Convolutional Neural Networks</div>

    <div class="materials-item">
      <a href="convolutional-networks/">
        Convolutional Neural Networks: Architectures, Convolution / Pooling Layers
      </a>
      <div class="kw">
          layers, spatial arrangement, layer patterns, layer sizing patterns, AlexNet/ZFNet/VGGNet case studies, computational considerations
      </div>
    </div>

    <div class="materials-item">
      <a href="understanding-cnn/">
        Understanding and Visualizing Convolutional Neural Networks
      </a>
      <div class="kw">
        tSNE embeddings, deconvnets, data gradients, fooling ConvNets, human comparisons
      </div>
    </div>

    <div class="materials-item">
      <a href="transfer-learning/">
        Transfer Learning and Fine-tuning Convolutional Neural Networks
      </a>
    </div>

  </div>
</div>
